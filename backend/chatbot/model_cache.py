# model_cache.py — Enhanced centralized model caching with memory management
import gc
import os
import threading
import time
from contextlib import contextmanager
from typing import Any, Dict, Optional

import psutil
from llama_cpp import Llama
from sentence_transformers import SentenceTransformer

# =============================================================================
# GLOBAL MODEL CACHES WITH MEMORY MANAGEMENT
# =============================================================================
_LLM_INSTANCE = None
_LLM_LOADED = False
_EMBEDDING_MODEL = None
_EMBEDDING_MODEL_LOADED = False
_MEMORY_MONITOR = None
_MEMORY_THRESHOLD = 0.85  # 85% memory usage threshold
_CACHE_LOCK = threading.Lock()
_MEMORY_STATS = {
    "llm_loads": 0,
    "embedding_loads": 0,
    "memory_cleanups": 0,
    "last_cleanup": 0
}

# =============================================================================
# LLM CACHING
# =============================================================================
def _monitor_memory() -> Dict[str, float]:
    """Monitor system memory usage"""
    try:
        memory = psutil.virtual_memory()
        return {
            "total_gb": memory.total / (1024**3),
            "available_gb": memory.available / (1024**3),
            "used_percent": memory.percent,
            "free_gb": memory.free / (1024**3)
        }
    except Exception:
        return {"error": "Unable to monitor memory"}

def _check_memory_pressure() -> bool:
    """Check if memory pressure requires cleanup"""
    memory_info = _monitor_memory()
    if "error" in memory_info:
        return False
    return memory_info["used_percent"] > (_MEMORY_THRESHOLD * 100)

def _force_cleanup():
    """Force garbage collection and memory cleanup"""
    global _MEMORY_STATS
    try:
        gc.collect()
        _MEMORY_STATS["memory_cleanups"] += 1
        _MEMORY_STATS["last_cleanup"] = time.time()
        print("🧹 Memory cleanup completed")
    except Exception as e:
        print(f"⚠️ Memory cleanup failed: {e}")

@contextmanager
def memory_managed_operation():
    """Context manager for memory-managed operations"""
    memory_before = _monitor_memory()
    try:
        yield
    finally:
        memory_after = _monitor_memory()
        if _check_memory_pressure():
            print("⚠️ High memory usage detected, forcing cleanup...")
            _force_cleanup()

def get_cached_llm() -> Optional[Llama]:
    """Get cached LLM instance with enhanced memory management"""
    global _LLM_INSTANCE, _LLM_LOADED, _MEMORY_STATS
    
    with _CACHE_LOCK:
        if _LLM_INSTANCE is not None:
            # Test if the instance is still valid
            try:
                _ = _LLM_INSTANCE.model_path
                return _LLM_INSTANCE
            except Exception:
                # If the instance is corrupted, clear it and reload
                print("🔄 LLM instance corrupted, clearing cache...")
                clear_llm_cache()
    
    if not _LLM_LOADED:
        with memory_managed_operation():
            try:
                print("Loading LLM model...")
                start_time = time.time()
                
                # Check memory before loading
                memory_before = _monitor_memory()
                print(f"📊 Memory before LLM load: {memory_before.get('used_percent', 0):.1f}%")
                
                # LLM Configuration
                BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
                MODEL_PATH = os.path.join(BASE_DIR, "law-chat.Q4_K_M.gguf")
                
                # Check if model file exists
                if not os.path.exists(MODEL_PATH):
                    raise FileNotFoundError(f"Model file not found: {MODEL_PATH}")
                
                # Enhanced CUDA configuration with memory management
                LLM_CONFIG = {
                    "model_path": MODEL_PATH,
                    "n_ctx": 4096,
                    "n_gpu_layers": -1,  # Use all GPU layers
                    "n_threads": 6,  # Reduced threads to prevent resource conflicts
                    "n_batch": 32,   # Further reduced batch size for stability
                    "n_ubatch": 16,  # Reduced ubatch for better memory management
                    "use_mmap": True,
                    "use_mlock": False,
                    "low_vram": True,
                    "f16_kv": True,
                    "logits_all": False,
                    "embedding": False,
                    "verbose": False,
                    "tfs_z": 1.0,    # Add tensor fusion for stability
                    "mirostat": 0,   # Disable mirostat for stability
                    "mirostat_eta": 0.1,
                    "mirostat_tau": 5.0,
                    "repeat_penalty": 1.1,
                    "repeat_last_n": 64,
                    "top_k": 40,
                    "top_p": 0.9,
                    "temperature": 0.7,
                }
            
                try:
                    _LLM_INSTANCE = Llama(**LLM_CONFIG)
                    load_time = time.time() - start_time
                    memory_after = _monitor_memory()
                    print(f"[SUCCESS] LLM model loaded in {load_time:.2f}s")
                    print(f"📊 Memory after LLM load: {memory_after.get('used_percent', 0):.1f}%")
                    _MEMORY_STATS["llm_loads"] += 1
                    _LLM_LOADED = True
                except Exception as e:
                    print(f"[ERROR] CUDA configuration failed: {e}")
                    print("🔄 Trying CPU-only fallback...")
                    
                    # Fallback to CPU-only configuration
                    cpu_config = {
                        "model_path": MODEL_PATH,
                        "n_ctx": 2048,  # Reduced context for CPU
                        "n_gpu_layers": 0,  # Force CPU-only
                        "n_threads": 4,
                        "n_batch": 16,  # Smaller batch for CPU
                        "use_mmap": True,
                        "use_mlock": False,
                        "verbose": False,
                    }
                    
                    try:
                        _LLM_INSTANCE = Llama(**cpu_config)
                        load_time = time.time() - start_time
                        memory_after = _monitor_memory()
                        print(f"[SUCCESS] LLM model loaded in CPU-only mode in {load_time:.2f}s")
                        print(f"📊 Memory after LLM load: {memory_after.get('used_percent', 0):.1f}%")
                        _MEMORY_STATS["llm_loads"] += 1
                        _LLM_LOADED = True
                    except Exception as e2:
                        print(f"[ERROR] CPU fallback also failed: {e2}")
                        _LLM_LOADED = True  # Mark as loaded to prevent retry loops
                        return None
            
            except Exception as e:
                print(f"[ERROR] Failed to load LLM model: {e}")
                _LLM_LOADED = True  # Mark as loaded to prevent retry loops
                return None
    
    return _LLM_INSTANCE

def clear_llm_cache():
    """Clear the LLM instance cache with enhanced memory management"""
    global _LLM_INSTANCE, _LLM_LOADED, _MEMORY_STATS
    
    with _CACHE_LOCK:
        memory_before = _monitor_memory()
        print(f"📊 Memory before LLM cache clear: {memory_before.get('used_percent', 0):.1f}%")
        
        if _LLM_INSTANCE is not None:
            try:
                # Try to properly close the model to avoid sampler attribute errors
                if hasattr(_LLM_INSTANCE, 'close'):
                    _LLM_INSTANCE.close()
            except Exception as e:
                # Ignore errors during cleanup, especially sampler attribute errors
                print(f"⚠️ Warning during LLM close: {e}")
            try:
                # Try to free the model from memory
                del _LLM_INSTANCE
            except Exception as e:
                # Ignore errors during deletion
                print(f"⚠️ Warning during LLM deletion: {e}")
        
        _LLM_INSTANCE = None
        _LLM_LOADED = False
        
        # Force garbage collection
        _force_cleanup()
        
        memory_after = _monitor_memory()
        print(f"📊 Memory after LLM cache clear: {memory_after.get('used_percent', 0):.1f}%")
        print("[CLEARED] LLM cache cleared with memory management")

# =============================================================================
# SENTENCE TRANSFORMER CACHING
# =============================================================================
def get_cached_embedding_model() -> Optional[SentenceTransformer]:
    """Get cached SentenceTransformer model with lazy loading"""
    global _EMBEDDING_MODEL, _EMBEDDING_MODEL_LOADED
    
    if _EMBEDDING_MODEL is None and not _EMBEDDING_MODEL_LOADED:
        print("🔄 Loading SentenceTransformer model...")
        start_time = time.time()
        
        # Use optimized model path
        model_name = os.getenv("EMBED_MODEL", "Stern5497/sbert-legal-xlm-roberta-base")
        device = "cuda" if os.getenv("USE_CUDA", "true").lower() == "true" else "cpu"
        
        _EMBEDDING_MODEL = SentenceTransformer(model_name, device=device)
        
        load_time = time.time() - start_time
        print(f"[SUCCESS] SentenceTransformer model loaded in {load_time:.2f}s")
        _EMBEDDING_MODEL_LOADED = True
    
    return _EMBEDDING_MODEL

def clear_embedding_model_cache():
    """Clear the SentenceTransformer model cache to free memory"""
    global _EMBEDDING_MODEL, _EMBEDDING_MODEL_LOADED
    if _EMBEDDING_MODEL is not None:
        try:
            # Try to free the model from memory
            del _EMBEDDING_MODEL
        except Exception as e:
            # Ignore errors during cleanup
            pass
    _EMBEDDING_MODEL = None
    _EMBEDDING_MODEL_LOADED = False
    print("[CLEARED] SentenceTransformer model cache cleared")

# =============================================================================
# COMBINED CACHE MANAGEMENT
# =============================================================================
def clear_all_model_caches():
    """Clear all model caches to free memory"""
    print("[CLEARING] All model caches...")
    clear_llm_cache()
    clear_embedding_model_cache()
    print("[SUCCESS] All model caches cleared")

def get_cache_status() -> dict:
    """Get status of all model caches"""
    return {
        "llm_loaded": _LLM_INSTANCE is not None,
        "llm_marked_loaded": _LLM_LOADED,
        "embedding_loaded": _EMBEDDING_MODEL is not None,
        "embedding_marked_loaded": _EMBEDDING_MODEL_LOADED,
    }

def print_cache_status():
    """Print current cache status with memory information"""
    status = get_cache_status()
    memory_info = _monitor_memory()
    
    print("MODEL CACHE STATUS:")
    print(f"  LLM Model: {'[LOADED]' if status['llm_loaded'] else '[NOT LOADED]'}")
    print(f"  Embedding Model: {'[LOADED]' if status['embedding_loaded'] else '[NOT LOADED]'}")
    print(f"  LLM Marked Loaded: {status['llm_marked_loaded']}")
    print(f"  Embedding Marked Loaded: {status['embedding_marked_loaded']}")
    print(f"  Memory Usage: {memory_info.get('used_percent', 0):.1f}%")
    print(f"  Available Memory: {memory_info.get('available_gb', 0):.1f} GB")
    print(f"  Memory Pressure: {'HIGH' if _check_memory_pressure() else 'NORMAL'}")

def get_memory_stats() -> Dict[str, Any]:
    """Get comprehensive memory statistics"""
    memory_info = _monitor_memory()
    return {
        **memory_info,
        "memory_pressure": _check_memory_pressure(),
        "cache_stats": _MEMORY_STATS.copy(),
        "threshold": _MEMORY_THRESHOLD
    }

def auto_cleanup_if_needed():
    """Automatically cleanup if memory pressure is high"""
    if _check_memory_pressure():
        print("⚠️ High memory pressure detected, performing automatic cleanup...")
        clear_all_model_caches()
        return True
    return False

def set_memory_threshold(threshold: float):
    """Set memory threshold for automatic cleanup (0.0 to 1.0)"""
    global _MEMORY_THRESHOLD
    if 0.0 <= threshold <= 1.0:
        _MEMORY_THRESHOLD = threshold
        print(f"Memory threshold set to {threshold * 100:.1f}%")
    else:
        print("Invalid threshold. Must be between 0.0 and 1.0")
